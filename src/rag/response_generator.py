"""
MEB RAG Sistemi - Response Generator
LLM-based response generation with structured output
"""
from typing import Optional, List, Dict, Any
from langchain_openai import AzureChatOpenAI

from config.settings import get_settings
from src.rag.output_models import AnalysisOutput, MatchedKazanim, PrerequisiteGap
from src.utils.resilience import with_resilience, CircuitOpenError


class ResponseGenerator:
    """
    Generates structured responses using LLM with Pydantic output.
    
    CRITICAL: Uses llm.with_structured_output() for guaranteed JSON!
    """
    
    SYSTEM_PROMPT = """Sen Yediiklim OkullarÄ±'nda gÃ¶rev yapan deneyimli bir fen bilimleri Ã¶ÄŸretmenisin.
Ã–ÄŸrencilerin sorularÄ±nÄ± MEB mÃ¼fredatÄ±na uygun ÅŸekilde, kavramsal anlayÄ±ÅŸÄ± geliÅŸtirerek yanÄ±tlÄ±yorsun.

ğŸ¯ TEMEL Ä°LKELER:
1. KAYNAK BAÄLILIÄI: Sadece verilen ders kitabÄ± iÃ§eriklerini kullan, bilgi uydurma
2. PEDAGOJÄ°K YAKLAÅIM: Ezber deÄŸil anlama odaklÄ± aÃ§Ä±kla
3. SEVÄ°YE UYUMU: Ã–ÄŸrencinin sÄ±nÄ±fÄ±na uygun dil ve karmaÅŸÄ±klÄ±k kullan
4. YANILGI FARKINDALIGI: YaygÄ±n Ã¶ÄŸrenci hatalarÄ±nÄ± belirt ve dÃ¼zelt

ğŸ“ MATEMATÄ°KSEL NOTASYON:
Ã‡Ã¶zÃ¼m adÄ±mlarÄ±nda matematiksel ifadeler iÃ§in LaTeX kullan:
- SatÄ±r iÃ§i: $formÃ¼l$ (Ã¶rn: $x^2 + 1$)
- Blok: $$formÃ¼l$$ (Ã¶rn: $$\\frac{a}{b}$$)
- YaygÄ±n semboller: $\\sqrt{x}$, $\\frac{a}{b}$, $\\sum$, $\\int$, $\\alpha$, $\\beta$, $\\pi$

âš ï¸ KRÄ°TÄ°K - KAYNAK BAÄLILIÄI:
- SADECE verilen ders kitabÄ± iÃ§eriklerinden bilgi kullan
- Kitapta YAZMAYAN bilgiyi KESÄ°NLÄ°KLE UYDURMA
- Emin olmadÄ±ÄŸÄ±n bilgi iÃ§in "Bu detay verilen kaynaklarda yer almÄ±yor" de
- Her bilgi iÃ§in sayfa numarasÄ± belirt (Ã–rn: "Biyoloji 11, s.117")

âš ï¸ YASAKLAR:
- Kitapta olmayan bilgiyi uydurma
- Belirsiz ifadeler kullanma ("muhtemelen", "sanÄ±rÄ±m", "galiba")
- Sayfa numarasÄ± vermeden kaynak gÃ¶sterme
- Soruyu yanÄ±tlamadan bÄ±rakma

âœ… ZORUNLULUKLAR:
- Her bilginin kaynaÄŸÄ±nÄ± belirt (Kitap adÄ±, sayfa numarasÄ±)
- "YanlÄ±ÅŸÄ± bul" sorularÄ±nda her seÃ§eneÄŸi tek tek deÄŸerlendir
- YanlÄ±ÅŸ seÃ§eneklerde NEDEN yanlÄ±ÅŸ olduÄŸunu aÃ§Ä±kla
- TÃ¼rkÃ§e yanÄ±t ver
- Ã–zet mesajÄ± Ã¶ÄŸrenci iÃ§in motive edici ve anlaÅŸÄ±lÄ±r olmalÄ±"""

    ANALYSIS_PROMPT = """## Soru
{question_text}

## Ã–ÄŸrenci Bilgisi
SÄ±nÄ±f: {grade}. sÄ±nÄ±f | Mod: {mode}

## Tespit Edilen Konular
{topics}

## EÅŸleÅŸen KazanÄ±mlar (Retrieval SonuÃ§larÄ±)
{kazanimlar}

## Ä°lgili Ders KitabÄ± BÃ¶lÃ¼mleri
{textbook_sections}

---

## GÃ–REV: Bu soruyu analiz et ve Ã§Ã¶z

### ğŸ“ Ã‡Ã–ZÃœM ADIMLARI (solution_steps - EN AZ 4 ADIM ZORUNLU!)

Her adÄ±mÄ± ÅŸu yapÄ±da oluÅŸtur:

**ADIM 1: VERÄ°LENLERÄ° BELÄ°RLE**
â†’ Soruda ne verilmiÅŸ? Ne isteniyor? Soru tipi nedir? (Ã§oktan seÃ§meli/aÃ§Ä±k uÃ§lu/yanlÄ±ÅŸÄ± bul)

**ADIM 2: Ä°LGÄ°LÄ° KAVRAMLARI HATIRLAT**
â†’ Bu konuyla ilgili temel bilgi nedir?
â†’ MUTLAKA ders kitabÄ±ndan alÄ±ntÄ± yap ve sayfa numarasÄ± ver!
â†’ Ã–rnek: "Ders kitabÄ±na gÃ¶re (Biyoloji 11, s.117): KapakÃ§Ä±klar basÄ±nÃ§ farkÄ±yla aÃ§Ä±lÄ±p kapanÄ±r."

**ADIM 3: SEÃ‡ENEKLERÄ°/DURUMU ANALÄ°Z ET**
â†’ Ã‡oktan seÃ§meli ise: Her seÃ§eneÄŸi AYRI AYRI deÄŸerlendir
â†’ "YanlÄ±ÅŸÄ± bul" ise: Her ifadenin DOÄRU/YANLIÅ olduÄŸunu belirt ve NEDEN olduÄŸunu aÃ§Ä±kla
â†’ AÃ§Ä±k uÃ§lu ise: Ã‡Ã¶zÃ¼m yolunu adÄ±m adÄ±m gÃ¶ster

**ADIM 4: SONUÃ‡ VE DOÄRULAMA**
â†’ Cevap nedir? MantÄ±klÄ± mÄ± kontrol et.
â†’ YaygÄ±n Ã¶ÄŸrenci yanÄ±lgÄ±sÄ± varsa uyar.

### ğŸ” "YANLIÅI BUL" SORULARI Ä°Ã‡Ä°N Ã–ZEL TALÄ°MAT:
Bu tip sorularda HER SEÃ‡ENEÄÄ° ÅŸu formatta deÄŸerlendir:
- A) [Ä°fade Ã¶zeti] â†’ âœ… DOÄRU / âŒ YANLIÅ - Ã‡Ã¼nkÃ¼: [AÃ§Ä±klama + Kaynak]
- B) [Ä°fade Ã¶zeti] â†’ âœ… DOÄRU / âŒ YANLIÅ - Ã‡Ã¼nkÃ¼: [AÃ§Ä±klama + Kaynak]
...

### âš ï¸ YANILGI UYARISI:
EÄŸer soruda yaygÄ±n bir Ã¶ÄŸrenci yanÄ±lgÄ±sÄ± varsa, bunu "summary" alanÄ±nda belirt.
Ã–rnek: "Dikkat: KapakÃ§Ä±klarÄ±n sinirle kontrol edildiÄŸi sÄ±k yapÄ±lan bir hatadÄ±r. AslÄ±nda basÄ±nÃ§ farkÄ±yla Ã§alÄ±ÅŸÄ±rlar."

### ğŸ“Š GÃœVENÄ°LÄ°RLÄ°K (confidence) DEÄERLENDÄ°RMESÄ°:
- 0.9+: Kitapta direkt cevap var, kazanÄ±m tam eÅŸleÅŸiyor
- 0.7-0.9: Kitapta ilgili iÃ§erik var ama dolaylÄ±
- 0.5-0.7: KÄ±smi eÅŸleÅŸme, yoruma aÃ§Ä±k
- <0.5: Kaynaklarda yeterli bilgi yok

### âœ… ZORUNLU Ã‡IKTI ALANLARI:
1. solution_steps: En az 4 adÄ±m (yukarÄ±daki yapÄ±da)
2. final_answer: Net cevap (Ã–rn: "C ÅŸÄ±kkÄ±", "42 cmÂ²")
3. matched_kazanimlar: TÃ¼m kazanÄ±mlarÄ± dahil et (match_type: primary/alternative)
4. textbook_references: KullandÄ±ÄŸÄ±n tÃ¼m kaynaklar (kitap adÄ±, sayfa)
5. summary: Ã–ÄŸrenci iÃ§in anlaÅŸÄ±lÄ±r, motive edici Ã¶zet
6. confidence: 0-1 arasÄ± gÃ¼venilirlik skoru

### EÅLEÅME NEDENÄ° FORMATI:
Her kazanÄ±m iÃ§in match_reason: "Bu kazanÄ±m [konu] ile ilgili Ã§Ã¼nkÃ¼ [sebep]. Kaynak: [Kitap, sayfa]"

EÄŸer soru bir gÃ¶rsel ise ve metin yoksa, gÃ¶rseldeki soruyu Ã§Ã¶zmeye Ã§alÄ±ÅŸ.
EÄŸer kaynaklarda yeterli bilgi yoksa, bunu aÃ§Ä±kÃ§a belirt ve confidence deÄŸerini dÃ¼ÅŸÃ¼k tut."""
    
    def __init__(self, llm: Optional[AzureChatOpenAI] = None):
        """
        Args:
            llm: Optional LangChain Azure ChatOpenAI. Creates one if not provided.
        """
        self.settings = get_settings()
        if llm:
            self.llm = llm
        else:
            self.llm = AzureChatOpenAI(
                azure_endpoint=self.settings.azure_openai_endpoint,
                api_key=self.settings.azure_openai_api_key,
                api_version=self.settings.azure_openai_api_version,
                azure_deployment=self.settings.azure_openai_chat_deployment,
                temperature=self.settings.llm_temperature_deterministic
            )

        # Create structured output chain
        self.structured_llm = self.llm.with_structured_output(AnalysisOutput)
    
    async def generate(
        self,
        question_text: str,
        matched_kazanimlar: List[Dict[str, Any]],
        related_chunks: List[Dict[str, Any]] = None,
        related_images: List[Dict[str, Any]] = None,
        detected_topics: List[str] = None,
        prerequisite_gaps: List[Dict[str, Any]] = None,
        grade: Optional[int] = None,
        is_exam_mode: bool = False
    ) -> AnalysisOutput:
        """
        Generate structured analysis response.

        Args:
            question_text: The student's question
            matched_kazanimlar: Retrieved kazanÄ±mlar from Phase 4
            related_chunks: Related textbook chunks
            related_images: Related textbook images
            detected_topics: Topics detected in the question
            prerequisite_gaps: Pre-computed prerequisite gaps
            grade: Student's grade level (9-12)
            is_exam_mode: Whether student is in YKS prep mode

        Returns:
            AnalysisOutput with structured response
        """
        from src.rag.output_models import ImageReference

        # Build prompt with grade context
        prompt = self._build_prompt(
            question_text=question_text,
            kazanimlar=matched_kazanimlar,
            textbook_sections=related_chunks or [],
            topics=detected_topics or [],
            grade=grade,
            is_exam_mode=is_exam_mode
        )
        
        try:
            # Generate with structured output and resilience
            result = await self._invoke_llm_with_resilience(prompt)
            
            # Add pre-computed gaps if any
            if prerequisite_gaps:
                result.prerequisite_gaps.extend([
                    PrerequisiteGap(**gap) for gap in prerequisite_gaps
                ])
                
            # Add retrieved images
            if related_images:
                result.image_references.extend([
                    ImageReference(
                        image_id=img.get("image_id") or img.get("id"),
                        caption=img.get("caption", ""),
                        page_number=img.get("page_number", 0),
                        why_relevant="GÃ¶rsel konuyla ilgili"  # Default reason
                    ) for img in related_images
                ])
            
            return result
            
        except CircuitOpenError as e:
            # Circuit is open - service is known to be down
            return AnalysisOutput(
                summary=f"Servis geÃ§ici olarak kullanÄ±lamÄ±yor. LÃ¼tfen daha sonra tekrar deneyin.",
                confidence=0.0
            )
        except Exception as e:
            # Return minimal valid output on error
            return AnalysisOutput(
                summary=f"Analiz sÄ±rasÄ±nda bir hata oluÅŸtu: {str(e)}",
                confidence=0.0
            )

    async def _invoke_llm_with_resilience(self, prompt: str) -> AnalysisOutput:
        """
        Invoke LLM with circuit breaker and retry protection.

        Uses the resilience module for:
        - Circuit breaker to fast-fail if Azure OpenAI is down
        - Exponential backoff retry for transient failures
        - Timeout protection
        """
        @with_resilience(
            circuit_name="azure_openai_response",
            timeout=self.settings.timeout_generate_response,
            use_circuit_breaker=True,
            use_retry=True
        )
        async def _invoke():
            return await self.structured_llm.ainvoke([
                {"role": "system", "content": self.SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ])

        return await _invoke()

    def generate_sync(
        self,
        question_text: str,
        matched_kazanimlar: List[Dict[str, Any]],
        related_chunks: List[Dict[str, Any]] = None,
        detected_topics: List[str] = None
    ) -> AnalysisOutput:
        """Synchronous version of generate"""
        prompt = self._build_prompt(
            question_text=question_text,
            kazanimlar=matched_kazanimlar,
            textbook_sections=related_chunks or [],
            topics=detected_topics or []
        )
        
        try:
            result = self.structured_llm.invoke([
                {"role": "system", "content": self.SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ])
            return result
        except Exception as e:
            return AnalysisOutput(
                summary=f"Analiz sÄ±rasÄ±nda bir hata oluÅŸtu: {str(e)}",
                confidence=0.0
            )
    
    def _build_prompt(
        self,
        question_text: str,
        kazanimlar: List[Dict[str, Any]],
        textbook_sections: List[Dict[str, Any]],
        topics: List[str],
        grade: Optional[int] = None,
        is_exam_mode: bool = False
    ) -> str:
        """Build the analysis prompt with detailed kazanÄ±m and textbook information"""
        # Format kazanÄ±mlar - Split by type
        primary_matches = []
        alternative_matches = []

        for k in kazanimlar:
            mclient = k.get("match_type", "primary")
            # Handle both string and Enum if mixed
            mtype = mclient.value if hasattr(mclient, "value") else str(mclient)

            if mtype == "alternative":
                alternative_matches.append(k)
            else:
                primary_matches.append(k)

        # 1. Primary Matches - WITH DETAILED INFO
        max_kazanimlar = self.settings.response_max_kazanimlar
        kazanim_text = "--- DOÄRUDAN EÅLEÅMELER (Ã‡Ã¶zÃ¼m Ä°Ã§in Kullan) ---\n"
        if primary_matches:
            for i, k in enumerate(primary_matches[:max_kazanimlar], 1):
                code = k.get("kazanim_code", "")
                title = k.get("kazanim_title", "")
                desc = k.get("kazanim_description", "")
                score = k.get("blended_score", k.get("score", 0))
                grade = k.get("grade", "?")
                subject = k.get("subject", "")
                # Get reranker reasoning if available
                match_reason = k.get("rerank_reasoning", "")

                kazanim_text += f"{i}. [{code}] {title or desc[:100]}\n"
                kazanim_text += f"   AÃ§Ä±klama: {desc}\n"
                kazanim_text += f"   Skor: {score:.2f} | SÄ±nÄ±f: {grade} | Ders: {subject}\n"
                if match_reason:
                    kazanim_text += f"   EÅŸleÅŸme Nedeni: {match_reason}\n"
                kazanim_text += "\n"
        else:
            kazanim_text += "DoÄŸrudan eÅŸleÅŸen kazanÄ±m bulunamadÄ±.\n"

        # 2. Alternative Matches
        if alternative_matches:
            kazanim_text += "\n--- ALTERNATÄ°F/Ä°LGÄ°LÄ° KAZANIMLAR (BaÄŸlam Ä°Ã§in Kullan) ---\n"
            for i, k in enumerate(alternative_matches[:max_kazanimlar], 1):
                code = k.get("kazanim_code", "")
                title = k.get("kazanim_title", "")
                desc = k.get("kazanim_description", "")
                grade = k.get("grade", "?")
                kazanim_text += f"- [{code}] (SÄ±nÄ±f {grade}) {title or desc}\n"

        # Format textbook sections - WITH FULL HIERARCHY
        max_sections = self.settings.response_max_textbook_sections
        content_truncate = self.settings.response_content_truncate
        sections_text = ""
        for i, s in enumerate(textbook_sections[:max_sections], 1):
            content = s.get("content", "")[:content_truncate]
            hierarchy = s.get("hierarchy_path", "")
            page_range = s.get("page_range", "")
            textbook_name = s.get("textbook_name", "Ders KitabÄ±")
            grade = s.get("grade", "")
            chunk_type = s.get("chunk_type", "")

            # Build full hierarchy string
            full_hierarchy = textbook_name
            if grade:
                full_hierarchy = f"{textbook_name} ({grade}. SÄ±nÄ±f)"
            if hierarchy:
                full_hierarchy += f" > {hierarchy}"
            if page_range:
                full_hierarchy += f" > Sayfa {page_range}"

            sections_text += f"{i}. [{full_hierarchy}]\n"
            if chunk_type:
                sections_text += f"   TÃ¼r: {chunk_type}\n"
            sections_text += f"   {content}\n\n"

        if not sections_text:
            sections_text = "Ä°lgili ders kitabÄ± bÃ¶lÃ¼mÃ¼ bulunamadÄ±."

        # Format topics
        topics_text = ", ".join(topics) if topics else "Konu tespit edilemedi"

        # Format grade and mode
        grade_text = str(grade) if grade else "BelirtilmemiÅŸ"
        mode_text = "YKS HazÄ±rlÄ±k" if is_exam_mode else "Okul"

        return self.ANALYSIS_PROMPT.format(
            question_text=question_text,
            grade=grade_text,
            mode=mode_text,
            topics=topics_text,
            kazanimlar=kazanim_text,
            textbook_sections=sections_text
        )
