"""
MEB RAG System - PDF Processing and Indexing Script

This script processes PDF textbooks from data/pdfs directory:
1. Analyzes layout using Azure Document Intelligence
2. Extracts images using PyMuPDF (fitz)
3. Generates semantic chunks preserving hierarchy
4. Indexes content and images to Azure AI Search

Usage:
    python scripts/process_pdfs.py
"""
import sys
import os
import asyncio
import glob
import json
import logging
from pathlib import Path
from dataclasses import asdict

# Configure logging to show retry attempts
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from azure.identity import DefaultAzureCredential, get_bearer_token_provider

from config.settings import get_settings
from src.document_processing.layout_analyzer import LayoutAnalyzer
from src.document_processing.image_extractor import ImageExtractor
from src.document_processing.semantic_chunker import SemanticChunker, SemanticChunk
from src.vector_store.indexing_pipeline import IndexingPipeline


import re
import uuid
from src.vector_store.question_generator import SyntheticQuestionGenerator


# ============== PROCESSED FILES TRACKING ==============
PROCESSED_STATE_FILE = Path("data/.processed_pdfs.json")


def load_processed_files() -> dict:
    """Load the set of already processed PDF files."""
    if PROCESSED_STATE_FILE.exists():
        try:
            with open(PROCESSED_STATE_FILE, "r") as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {"kazanim": [], "kitap": []}
    return {"kazanim": [], "kitap": []}


def save_processed_file(pdf_path: Path, file_type: str):
    """Mark a PDF as processed."""
    state = load_processed_files()
    pdf_key = str(pdf_path.absolute())

    if file_type not in state:
        state[file_type] = []

    if pdf_key not in state[file_type]:
        state[file_type].append(pdf_key)

        # Ensure parent directory exists
        PROCESSED_STATE_FILE.parent.mkdir(parents=True, exist_ok=True)

        with open(PROCESSED_STATE_FILE, "w") as f:
            json.dump(state, f, indent=2)
        print(f"   ðŸ’¾ Saved progress: {pdf_path.name}")


def is_already_processed(pdf_path: Path, file_type: str) -> bool:
    """Check if a PDF has already been processed."""
    state = load_processed_files()
    pdf_key = str(pdf_path.absolute())
    return pdf_key in state.get(file_type, [])


def clear_processed_state(mode: str = "all"):
    """Clear processed files state based on mode."""
    if mode == "all":
        if PROCESSED_STATE_FILE.exists():
            PROCESSED_STATE_FILE.unlink()
            print("ðŸ—‘ï¸  Cleared ALL processed files state")
    else:
        state = load_processed_files()
        if mode in state:
            state[mode] = []
            with open(PROCESSED_STATE_FILE, "w") as f:
                json.dump(state, f, indent=2)
            print(f"ðŸ—‘ï¸  Cleared {mode} processed files state")


def parse_filename(filename: str) -> dict:
    """
    Parse filename to extract metadata.
    Expected formats:
    - subject_grade.pdf (e.g., biyoloji_9.pdf)
    - subject_grade.semester.pdf (e.g., biyoloji_9.1.pdf, biyoloji_10.2.pdf)
    """
    import hashlib

    stem = Path(filename).stem  # e.g., "biyoloji_9.1" or "biyoloji_9"

    # Generate DETERMINISTIC numeric textbook_id using MD5
    # Python's hash() is not deterministic across runs/systems!
    md5_hash = hashlib.md5(stem.encode('utf-8')).hexdigest()
    textbook_id_hash = int(md5_hash[:8], 16) % (10**8)  # 8-digit positive integer
    
    metadata = {
        "subject": "genel",
        "grade": 0,
        "semester": 0,  # 0 = unknown, 1 = 1. dÃ¶nem, 2 = 2. dÃ¶nem
        "textbook_id": textbook_id_hash,
        "textbook_name": stem  # Keep original name for reference
    }
    
    # Split by underscore to get subject and grade parts
    parts = stem.split('_')
    
    if len(parts) >= 2:
        metadata["subject"] = parts[0]  # e.g., "biyoloji"
        
        # The grade part might be "9" or "9.1" (grade.semester)
        grade_part = parts[1]  # e.g., "9" or "9.1"
        
        if '.' in grade_part:
            # Format: subject_grade.semester.pdf (e.g., biyoloji_9.1.pdf)
            grade_semester = grade_part.split('.')
            try:
                metadata["grade"] = int(grade_semester[0])
                metadata["semester"] = int(grade_semester[1])
            except (ValueError, IndexError):
                pass
        else:
            # Format: subject_grade.pdf (e.g., biyoloji_9.pdf)
            try:
                metadata["grade"] = int(grade_part)
            except ValueError:
                pass
            
    return metadata


async def process_kazanim_pdf(
    pdf_path: Path,
    settings,
    doc_client,
    pipeline
):
    print(f"\nðŸ“‘ Processing Kazanim File: {pdf_path.name}")
    
    # Extract semester from filename
    file_metadata = parse_filename(pdf_path.name)
    semester = file_metadata.get("semester", 0)
    print(f"   â””â”€ Semester: {semester if semester > 0 else 'Unknown'}")
    
    # 1. Read PDF
    with open(pdf_path, "rb") as f:
        pdf_bytes = f.read()
    
    # 2. Extract Text (Simple Layout Analysis)
    print("   â””â”€ Extracting text (Azure Document Intelligence)...")
    analyzer = LayoutAnalyzer()
    result = await analyzer.analyze_document(doc_client, pdf_bytes)
    
    # Simple text extraction for regex parsing
    full_text = ""
    if result.content:
        full_text = result.content
    
    print(f"   â””â”€ Extracted Text Snippet: {full_text[:500]!r}")
    
    # 3. Parse Kazanim Codes with Regex
    # Format: BÄ°Y.9.1.1. Title text
    #         a) sub-item
    #         b) sub-item
    #         BÄ°Y.9.1.2. Next kazanim...
    print("   â””â”€ Parsing kazanim codes...")

    # NEW APPROACH: Parse sub-items (a, b, c, Ã§, d) separately for granular indexing
    # Each sub-item is a distinct, measurable learning outcome

    # Step 1: Find all main kazanÄ±m codes and their blocks
    # Pattern captures: CODE + everything until next CODE
    kazanim_block_pattern = r"([A-ZÄ°ÄžÃœÅžÃ–Ã‡]+\.\d+(?:\.\d+)+)\.?\s+([\s\S]+?)(?=\n[A-ZÄ°ÄžÃœÅžÃ–Ã‡]+\.\d+(?:\.\d+)+\.|$)"
    blocks = re.findall(kazanim_block_pattern, full_text)

    print(f"   â””â”€ Found {len(blocks)} kazanim blocks")

    if not blocks:
        print("   âš ï¸ No kazanim codes found! check regex or pdf format.")
        return

    # Step 2: Parse each block to extract sub-items (a, b, c, Ã§, d)
    # Sub-item pattern: letter followed by ) and text until next sub-item or double newline or new kazanim
    sub_item_pattern = r"([a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼])\)\s*(.+?)(?=\n[a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼]\)|\n\n|\n[A-ZÄ°ÄžÃœÅžÃ–Ã‡]+\.\d|$)"

    kazanim_dicts = []
    total_sub_items = 0

    for code, block_content in blocks:
        # Extract the title (first line before any sub-items)
        title_match = re.match(r"^([^\n]+?)(?=\n[a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼]\)|$)", block_content.strip())
        title = title_match.group(1).strip() if title_match else block_content.split('\n')[0].strip()

        # Parse code parts
        parts = code.split('.')
        grade = 0
        if len(parts) > 1 and parts[1].isdigit():
            grade = int(parts[1])
        subject = parts[0]  # 'BÄ°Y', 'M', etc.

        # Find all sub-items in this block (DOTALL allows . to match newlines)
        sub_items = re.findall(sub_item_pattern, block_content, re.DOTALL)

        if sub_items:
            # Index each sub-item separately with parent code reference
            for letter, content in sub_items:
                content = content.strip().replace("\n", " ").replace("  ", " ")
                if len(content) < 10:  # Skip empty/invalid sub-items
                    continue

                sub_code = f"{code}.{letter}"  # e.g., BÄ°Y.9.1.1.a
                total_sub_items += 1

                kazanim_dicts.append({
                    "id": str(uuid.uuid4()),
                    "code": sub_code,
                    "parent_code": code,
                    "description": content,
                    "title": title,  # Parent kazanÄ±m title for context
                    "grade": grade,
                    "subject": subject,
                    "semester": semester
                })
        else:
            # No sub-items found, index the main kazanÄ±m title
            description = title if len(title) < 500 else title[:500]
            kazanim_dicts.append({
                "id": str(uuid.uuid4()),
                "code": code,
                "parent_code": code,
                "description": description,
                "title": title,
                "grade": grade,
                "subject": subject,
                "semester": semester
            })

    print(f"   â””â”€ Parsed {total_sub_items} sub-items from {len(blocks)} kazanim blocks")
    print(f"   â””â”€ Total kazanÄ±mlar to index: {len(kazanim_dicts)}")

    # Debug: Show sample parsed items
    if kazanim_dicts:
        sample = kazanim_dicts[0]
        print(f"   â””â”€ Sample: {sample['code']} - {sample['description'][:80]}...")

    # 4. INDEX KAZANIMLAR TO PRIMARY INDEX (This is the main curriculum data!)
    print("   â””â”€ Indexing kazanÄ±mlar to PRIMARY index (meb-kazanimlar-index)...")
    pipeline.index_kazanimlar_raw(kazanim_dicts)

    # 5. Generate Synthetic Questions for each Kazanim (PARALLEL)
    # These are used for example question generation, NOT for curriculum retrieval
    print(f"   â””â”€ Generating synthetic questions (GPT-4o) - {len(kazanim_dicts)} kazanÄ±mlar...")
    generator = SyntheticQuestionGenerator()

    # Parallel generation with semaphore to respect rate limits
    PARALLEL_LIMIT = 5  # Process 5 at a time to avoid rate limits
    semaphore = asyncio.Semaphore(PARALLEL_LIMIT)

    async def generate_with_limit(kazanim):
        async with semaphore:
            print(f"      â””â”€ Generating questions for {kazanim['code']}...")
            return await generator.generate_for_kazanim_async(kazanim, count=5)

    # Run all in parallel
    tasks = [generate_with_limit(k) for k in kazanim_dicts]
    results = await asyncio.gather(*tasks)

    # Flatten results
    all_questions = []
    for qs in results:
        all_questions.extend(qs)

    print(f"   â””â”€ Generated {len(all_questions)} total synthetic questions")

    # 6. Index Synthetic Questions (secondary index for example questions)
    if all_questions:
        print("   â””â”€ Indexing synthetic questions to meb-sentetik-sorular-index...")

        # Build lookup for kazanim metadata
        kazanim_lookup = {k["code"]: k for k in kazanim_dicts}

        # Convert dataclass to dict for indexing
        q_dicts = []
        for q in all_questions:
            # Get the kazanim metadata for this question
            kaz = kazanim_lookup.get(q.parent_kazanim_code, {})
            q_dicts.append({
                "id": str(uuid.uuid4()),
                "question_text": q.question_text,
                "difficulty": q.difficulty,
                "question_type": q.question_type,
                "code": q.parent_kazanim_code,  # pipeline expects 'code'
                "description": kaz.get("description", ""),  # pipeline expects 'description'
                "grade": kaz.get("grade", 0),
                "subject": kaz.get("subject", ""),
                "semester": kaz.get("semester", 0)
            })

        pipeline.index_kazanimlar(q_dicts, generate_questions=False)  # Skip internal generation

    # Mark as processed AFTER successful completion
    save_processed_file(pdf_path, "kazanim")
    print(f"[OK] Completed Kazanim File: {pdf_path.name}")


async def process_single_pdf(
    pdf_path: Path, 
    settings, 
    doc_client, 
    vision_client,
    pipeline
):
    # Check if this is a kazanim file
    if "kazanim" in str(pdf_path).lower() or "kazanÄ±m" in str(pdf_path).lower():
        await process_kazanim_pdf(pdf_path, settings, doc_client, pipeline)
        return

    print(f"\nðŸ“˜ Processing Textbook: {pdf_path.name}")
    
    # 1. Read PDF
    with open(pdf_path, "rb") as f:
        pdf_bytes = f.read()
    
    metadata = parse_filename(pdf_path.name)
    print(f"   â””â”€ Metadata: {metadata}")

    # 2. Layout Analysis with retry support for large PDFs
    file_size_mb = len(pdf_bytes) / (1024 * 1024)
    print(f"   â””â”€ Analyzing layout (Azure Document Intelligence)... [{file_size_mb:.1f} MB]")
    if file_size_mb > 50:
        print(f"   â””â”€ Large file detected - may take several minutes with auto-retry on timeout")

    analyzer = LayoutAnalyzer()
    result = await analyzer.analyze_document(doc_client, pdf_bytes, max_retries=3, initial_delay=30.0)
    elements = analyzer.classify_elements(result)
    print(f"   â””â”€ Found {len(elements)} layout elements")
    if result.figures:
        print(f"   â””â”€ Azure found {len(result.figures)} figures in total")
    else:
        print(f"   â””â”€ Azure found 0 figures")

    # 3. Image Extraction
    print("   â””â”€ Extracting images...")
    # Convert textbook_id to string for path composition
    img_output_dir = Path("data/processed/images") / str(metadata["textbook_id"])
    extractor = ImageExtractor(vision_client=vision_client, output_dir=img_output_dir)
    
    # Extract images using Azure coordinates
    # Note: LayoutAnalyzer returns full result which contains figures
    images = extractor.extract_images_from_bytes(pdf_bytes, result.figures or [])
    print(f"   â””â”€ Extracted {len(images)} valid images")
    
    # Generate captions for images (Async)
    if images and vision_client:
        print("   â””â”€ Generating image captions (GPT-4o Vision)...")
        images = await extractor.generate_captions(images)
        
        # Log summary of types
        type_counts = {}
        for img in images:
            t = img.image_type or "unknown"
            type_counts[t] = type_counts.get(t, 0) + 1
        print(f"   â””â”€ Image Analysis Summary: {len(images)} images -> {type_counts}")

    # 4. Semantic Chunking
    print("   â””â”€ Creating semantic chunks...")
    chunker = SemanticChunker()
    chunks = chunker.chunk_document(elements)
    chunks = chunker.merge_small_chunks(chunks)
    print(f"   â””â”€ Created {len(chunks)} chunks")

    # 5. Indexing
    print("   â””â”€ Indexing to Azure AI Search...")

    # Helper: Extract chapter number from hierarchy_path
    def extract_chapter_id(hierarchy_path: str) -> int:
        """Extract chapter/unit number from hierarchy path like 'Ãœnite 1/Konu 2'"""
        if not hierarchy_path:
            return 0
        import re
        # Match patterns like "Ãœnite 1", "BÃ¶lÃ¼m 2", "Unit 3", "1. Ãœnite"
        patterns = [
            r'[ÃœÃ¼]nite\s*(\d+)',
            r'[Bb]Ã¶lÃ¼m\s*(\d+)',
            r'[Uu]nit\s*(\d+)',
            r'^(\d+)\.\s*[ÃœÃ¼]nite',
        ]
        for pattern in patterns:
            match = re.search(pattern, hierarchy_path, re.IGNORECASE)
            if match:
                return int(match.group(1))
        return 0

    # Build page-to-chunk mapping for linking images to chunks
    page_to_chunks = {}
    for chunk in chunks:
        start_page, end_page = chunk.page_range
        for page in range(start_page, end_page + 1):
            if page not in page_to_chunks:
                page_to_chunks[page] = []
            page_to_chunks[page].append(chunk)

    # Prepare chunk dicts
    chunk_dicts = []
    for chunk in chunks:
        c_dict = {
            "id": chunk.chunk_id,
            "content": chunk.content,
            "chunk_type": chunk.chunk_type,
            "hierarchy_path": chunk.hierarchy_path,
            "page_range": f"{chunk.page_range[0]}-{chunk.page_range[1]}",
            "is_sidebar": chunk.is_sidebar_content,
            "textbook_id": metadata["textbook_id"],
            "textbook_name": metadata["textbook_name"],
            "chapter_id": extract_chapter_id(chunk.hierarchy_path),  # NEW: Extract chapter number
            "grade": metadata["grade"],
            "subject": metadata["subject"],
            "semester": metadata["semester"],
        }
        chunk_dicts.append(c_dict)

    # Prepare image dicts with chunk linkage
    image_dicts = []
    for img in images:
        # Find the best matching chunk for this image based on page number
        matching_chunks = page_to_chunks.get(img.page_number, [])

        # Get the first non-sidebar chunk, or any chunk if none available
        best_chunk = None
        for c in matching_chunks:
            if not c.is_sidebar_content:
                best_chunk = c
                break
        if not best_chunk and matching_chunks:
            best_chunk = matching_chunks[0]

        # Extract related text (first 300 chars of matching chunk)
        related_text = ""
        chunk_id = ""
        hierarchy_path = img.hierarchy_path or ""

        if best_chunk:
            chunk_id = best_chunk.chunk_id
            hierarchy_path = best_chunk.hierarchy_path or hierarchy_path
            # Get a preview of the related text (clean it up)
            content_preview = best_chunk.content[:300].replace('\n', ' ').strip()
            if len(best_chunk.content) > 300:
                content_preview += "..."
            related_text = content_preview

        i_dict = {
            "id": img.image_id,
            "caption": img.caption or "GÃ¶rsel",
            "image_type": img.image_type or "unknown",
            "page_number": img.page_number,
            "chunk_id": chunk_id,  # NEW: Link to chunk
            "related_text": related_text,  # NEW: Context from chunk
            "hierarchy_path": hierarchy_path,  # NEW: Inherit from chunk if not set
            "image_path": str(img.image_path) if img.image_path else "",
            "width": img.width if hasattr(img, 'width') else 0,
            "height": img.height if hasattr(img, 'height') else 0,
            "textbook_id": metadata["textbook_id"],
            "grade": metadata["grade"],
            "subject": metadata["subject"]
        }
        image_dicts.append(i_dict)

    print(f"   â””â”€ Linked {sum(1 for i in image_dicts if i['chunk_id'])} images to chunks")

    # Convert sync methods to async run where necessary or run directly if pipeline supports it
    # ensure pipeline uses correct index names
    
    # Index Chunks
    if chunk_dicts:
        pipeline.index_textbook_chunks(chunk_dicts)
    
    # Index Images
    if image_dicts:
        pipeline.index_images(image_dicts)

    # Mark as processed AFTER successful completion
    save_processed_file(pdf_path, "kitap")
    print(f"[OK] Completed Textbook: {pdf_path.name}")


async def main(process_mode: str = "all", reset: str = None):
    """
    Main processing function.

    Args:
        process_mode: "all", "kazanim", or "kitap" - determines which PDFs to process
        reset: Index deletion mode - "all", "kazanim", "kitap", or None (no deletion)
    """
    print("="*60)
    print("MEB RAG - PDF Ingestion Pipeline")
    print(f"Mode: {process_mode.upper()}")
    print("="*60)
    
    settings = get_settings()
    
    # ... (Directories check omitted for brevity in replace, assume handled or no change needed to top part)
    # Actually I need to keep the function body intact or be careful with replace range.
    # I will target specific blocks.

    # Validate Directories
    pdf_dir = Path("data/pdfs")
    if not pdf_dir.exists():
        print(f"[ERROR] Directory not found: {pdf_dir}")
        return

    # Check Azure Keys
    if not settings.documentintelligence_api_key or not settings.azure_search_api_key:
        print("[ERROR] Missing Azure API Keys in .env")
        return

    # Initialize Clients
    print("[INFO] Initializing Azure Clients...")

    # Document Intelligence Client with extended timeouts for large PDFs
    # Use retry_total and per_retry_timeout for robust handling of large files
    from azure.core.pipeline.policies import RetryPolicy

    doc_client = DocumentIntelligenceClient(
        endpoint=settings.documentintelligence_endpoint,
        credential=AzureKeyCredential(settings.documentintelligence_api_key),
        retry_total=5,  # Total retry attempts
        retry_backoff_factor=2,  # Exponential backoff
        retry_backoff_max=120,  # Max 2 minutes between retries
    )
    
    # Azure OpenAI Client (for Vision)
    from openai import AsyncAzureOpenAI
    vision_client = AsyncAzureOpenAI(
        api_key=settings.azure_openai_api_key,
        api_version=settings.azure_openai_api_version,
        azure_endpoint=settings.azure_openai_endpoint
    )

    # Indexing Pipeline
    pipeline = IndexingPipeline()
    
    # RESET IF REQUESTED
    if reset:
        # Pass the reset mode (can be different from process_mode)
        pipeline.delete_indexes(reset)
        # Also clear the processed files state
        clear_processed_state(reset)

    # Ensure indexes exist
    pipeline.create_all_indexes()
    
    # Find PDFs based on mode
    if process_mode == "kazanim":
        pdf_files = list(Path("data/pdfs/kazanimlar").rglob("*.pdf"))
        print(f"ðŸ“‘ Sadece kazanÄ±mlar iÅŸlenecek: {len(pdf_files)} dosya")
    elif process_mode == "kitap":
        pdf_files = list(Path("data/pdfs/ders_kitaplari").rglob("*.pdf"))
        print(f"ðŸ“˜ Sadece ders kitaplarÄ± iÅŸlenecek: {len(pdf_files)} dosya")
    else:
        pdf_files = list(pdf_dir.rglob("*.pdf"))
        print(f"ðŸ“š TÃ¼m PDF'ler iÅŸlenecek: {len(pdf_files)} dosya")
    
    if not pdf_files:
        print("âš ï¸  No PDF files found!")
        return

    # Check for already processed files
    skipped_count = 0
    to_process = []

    for pdf_file in pdf_files:
        # Determine file type based on path or content
        is_kazanim = "kazanim" in str(pdf_file).lower() or "kazanÄ±m" in str(pdf_file).lower()
        file_type = "kazanim" if is_kazanim else "kitap"

        if is_already_processed(pdf_file, file_type):
            skipped_count += 1
            print(f"â­ï¸  Skipping (already processed): {pdf_file.name}")
        else:
            to_process.append(pdf_file)

    if skipped_count > 0:
        print(f"\nðŸ“Š {skipped_count} dosya zaten iÅŸlenmiÅŸ, {len(to_process)} dosya iÅŸlenecek\n")

    if not to_process:
        print("âœ… TÃ¼m dosyalar zaten iÅŸlenmiÅŸ!")
        return

    # Process each PDF
    for i, pdf_file in enumerate(to_process, 1):
        print(f"\n[{i}/{len(to_process)}] Processing...")
        try:
            await process_single_pdf(
                pdf_file,
                settings,
                doc_client,
                vision_client,
                pipeline
            )
        except Exception as e:
            print(f"[ERROR] Error processing {pdf_file.name}: {e}")
            import traceback
            traceback.print_exc()
            # Don't mark as processed if there was an error!

    print("\n[DONE] All processing completed!")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="MEB RAG - PDF Processing and Indexing",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ã–rnekler:
  python scripts/process_pdfs.py                    # TÃ¼m PDF'leri iÅŸle
  python scripts/process_pdfs.py --reset            # Ã–nce tÃ¼m indexleri sil sonra iÅŸle
  python scripts/process_pdfs.py -k -r              # Sadece kazanÄ±mlarÄ± iÅŸle ve Ã¶nce sil
  python scripts/process_pdfs.py -t --reset-kitap   # KitaplarÄ± iÅŸle, sadece kitap indexini sil
  python scripts/process_pdfs.py --reset-kitap      # Sadece kitap indexini sil (iÅŸlem yapmadan)
  python scripts/process_pdfs.py --reset-kazanim    # Sadece kazanÄ±m indexini sil (iÅŸlem yapmadan)
        """
    )

    # Processing mode (what to process)
    process_group = parser.add_mutually_exclusive_group()
    process_group.add_argument(
        "--kazanim", "-k",
        action="store_true",
        help="Sadece kazanÄ±m PDF'lerini iÅŸle (data/pdfs/kazanimlar/)"
    )
    process_group.add_argument(
        "--kitap", "-t",
        action="store_true",
        help="Sadece ders kitaplarÄ±nÄ± iÅŸle (data/pdfs/ders_kitaplari/)"
    )

    # Reset options (what to delete)
    reset_group = parser.add_mutually_exclusive_group()
    reset_group.add_argument(
        "--reset", "-r",
        action="store_true",
        help="TÃ¼m indexleri SÄ°L ve baÅŸtan oluÅŸtur."
    )
    reset_group.add_argument(
        "--reset-kitap",
        action="store_true",
        help="Sadece kitap (chunks + images) indexlerini SÄ°L."
    )
    reset_group.add_argument(
        "--reset-kazanim",
        action="store_true",
        help="Sadece kazanÄ±m/soru indexini SÄ°L."
    )

    parser.add_argument(
        "--delete-only",
        action="store_true",
        help="Sadece index sil, PDF iÅŸleme yapma (--reset-* ile birlikte kullan)."
    )

    args = parser.parse_args()

    # Determine processing mode
    if args.kazanim:
        mode = "kazanim"
    elif args.kitap:
        mode = "kitap"
    else:
        mode = "all"

    # Determine reset mode
    if args.reset:
        reset_mode = mode  # Reset matches processing mode
    elif args.reset_kitap:
        reset_mode = "kitap"
    elif args.reset_kazanim:
        reset_mode = "kazanim"
    else:
        reset_mode = None

    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    # Handle delete-only mode
    if args.delete_only:
        if not reset_mode:
            print("[ERROR] --delete-only requires a reset flag (--reset, --reset-kitap, or --reset-kazanim)")
            sys.exit(1)
        # Just delete indexes without processing
        from src.vector_store.indexing_pipeline import IndexingPipeline
        pipeline = IndexingPipeline()
        pipeline.delete_indexes(reset_mode)
        print("[DONE] Index deletion completed.")
    else:
        asyncio.run(main(mode, reset=reset_mode))

