"""
KazanÄ±m Ä°ndexleme Pipeline (Unified)

Bu script tek bir pipeline ile:
1. data/kazanimlar_txt/ klasÃ¶rÃ¼ndeki temiz TXT dosyalarÄ±nÄ± okur
2. Her kazanÄ±m iÃ§in kitap index'inde arama yapar
3. LLM ile anahtar kelimeler Ã§Ä±karÄ±r (keyword enrichment)
4. ZenginleÅŸtirilmiÅŸ kazanÄ±mlarÄ± Azure Search'e indexler

Pipeline:
    TXT Files â†’ Parse â†’ Textbook Search â†’ LLM Keywords â†’ Enriched Index

KullanÄ±m:
    python scripts/index_kazanimlar_pipeline.py
    python scripts/index_kazanimlar_pipeline.py --reset          # Ã–nce indexi sil
    python scripts/index_kazanimlar_pipeline.py --subject BÄ°Y    # Sadece biyoloji
    python scripts/index_kazanimlar_pipeline.py --no-enrich      # Enrichment atla (hÄ±zlÄ±)
    python scripts/index_kazanimlar_pipeline.py --dry-run        # Test modu
"""

import sys
import os
import re
import json
import asyncio
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import get_settings
from config.azure_config import get_search_client
from langchain_openai import AzureChatOpenAI


# ============================================================================
# DATA CLASSES
# ============================================================================

@dataclass
class Kazanim:
    """Tek bir kazanÄ±m."""
    code: str
    description: str
    parent_code: str
    grade: int
    subject: str
    semester: int
    keywords: List[str] = field(default_factory=list)
    enriched_description: str = ""
    textbook_sources: List[str] = field(default_factory=list)


@dataclass
class PipelineStats:
    """Pipeline istatistikleri."""
    total_files: int = 0
    total_kazanimlar: int = 0
    enriched_count: int = 0
    failed_enrichment: int = 0
    indexed_count: int = 0
    start_time: datetime = field(default_factory=datetime.now)

    def elapsed_seconds(self) -> float:
        return (datetime.now() - self.start_time).total_seconds()


# ============================================================================
# CONSTANTS & MAPPINGS
# ============================================================================

# KazanÄ±m subject kodu â†’ Kitap index subject adÄ±
SUBJECT_CODE_TO_NAME = {
    "BÄ°Y": "biyoloji",
    "M": "matematik",
    "FÄ°Z": "fizik",
    "KÄ°M": "kimya",
    "T": "turkce",
    "TAR": "tarih",
    "COÄ": "cografya",
}

# Dosya adÄ± â†’ subject/grade mapping
FILENAME_SUBJECT_MAP = {
    "biyoloji": ("Biyoloji", "BÄ°Y"),
    "matematik": ("Matematik", "M"),
    "fizik": ("Fizik", "FÄ°Z"),
    "kimya": ("Kimya", "KÄ°M"),
    "turkce": ("TÃ¼rkÃ§e", "T"),
    "tarih": ("Tarih", "TAR"),
    "cografya": ("CoÄŸrafya", "COÄ"),
}


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def sanitize_id(code: str) -> str:
    """Azure Search iÃ§in geÃ§erli ID oluÅŸtur."""
    code_id = code.replace(".", "_").replace("-", "_")
    # TÃ¼rkÃ§e karakterleri deÄŸiÅŸtir (Ã§ â†’ cc to avoid collision with c)
    for tr_char, en_char in [("Ä°", "I"), ("Ä±", "i"), ("Ä", "G"), ("ÄŸ", "g"),
                              ("Ãœ", "U"), ("Ã¼", "u"), ("Å", "S"), ("ÅŸ", "s"),
                              ("Ã–", "O"), ("Ã¶", "o"), ("Ã‡", "CC"), ("Ã§", "cc")]:
        code_id = code_id.replace(tr_char, en_char)
    return code_id


def get_textbook_subject(kazanim_subject: str) -> str:
    """KazanÄ±m subject kodunu kitap index subject adÄ±na Ã§evir."""
    return SUBJECT_CODE_TO_NAME.get(kazanim_subject, kazanim_subject.lower())


def parse_metadata_from_filename(filename: str) -> dict:
    """Dosya adÄ±ndan metadata Ã§Ä±kar."""
    stem = Path(filename).stem.replace("_kazanimlar", "")
    parts = stem.split('_')

    metadata = {
        "subject": "?",
        "subject_code": "?",
        "grade": 0,
        "semester": 0
    }

    if len(parts) >= 2:
        subject_lower = parts[0].lower()
        if subject_lower in FILENAME_SUBJECT_MAP:
            metadata["subject"], metadata["subject_code"] = FILENAME_SUBJECT_MAP[subject_lower]
        else:
            metadata["subject"] = parts[0].capitalize()
            metadata["subject_code"] = parts[0][:3].upper()

        grade_part = parts[1]
        if '.' in grade_part:
            grade_semester = grade_part.split('.')
            try:
                metadata["grade"] = int(grade_semester[0])
                metadata["semester"] = int(grade_semester[1])
            except (ValueError, IndexError):
                pass
        else:
            try:
                metadata["grade"] = int(grade_part)
            except ValueError:
                pass

    return metadata


# ============================================================================
# TXT PARSING
# ============================================================================

def parse_txt_file(txt_path: Path, metadata: dict) -> List[Kazanim]:
    """
    Temiz txt dosyasÄ±nÄ± parse et ve Kazanim listesi dÃ¶ndÃ¼r.

    Format:
        BÄ°Y.10.1.1. Ana kazanÄ±m baÅŸlÄ±ÄŸÄ±
            a) Alt kazanÄ±m aÃ§Ä±klamasÄ±
            b) Alt kazanÄ±m aÃ§Ä±klamasÄ±
    """
    kazanimlar = []

    with open(txt_path, 'r', encoding='utf-8') as f:
        content = f.read()

    lines = content.split('\n')
    current_main = None
    current_main_title = None
    in_content = False

    for line in lines:
        line_stripped = line.strip()

        # Header bÃ¶lÃ¼mÃ¼nÃ¼ atla
        if line_stripped.startswith("="):
            in_content = True
            continue

        if not in_content:
            continue

        # Ã–ZET bÃ¶lÃ¼mÃ¼ne geldiÄŸinde dur
        if line_stripped == "Ã–ZET":
            break

        # Ana kazanÄ±m: BÄ°Y.10.1.1. BaÅŸlÄ±k
        main_match = re.match(r'^([A-ZÄ°ÄÃœÅÃ–Ã‡]+\.\d+\.\d+\.\d+)\.?\s*(.+)$', line_stripped)
        if main_match:
            current_main = main_match.group(1)
            current_main_title = main_match.group(2).strip()

            # Ana kazanÄ±mÄ± ekle
            kazanimlar.append(Kazanim(
                code=current_main,
                description=current_main_title,
                parent_code=current_main,
                grade=metadata['grade'],
                subject=metadata['subject_code'],
                semester=metadata['semester']
            ))
            continue

        # Alt kazanÄ±m: a) AÃ§Ä±klama
        if line.startswith("    ") and kazanimlar:
            sub_match = re.match(r'^\s+([a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼])\)\s*(.+)$', line)
            if sub_match:
                letter = sub_match.group(1)
                description = sub_match.group(2).strip()
                sub_code = f"{current_main}.{letter}"

                kazanimlar.append(Kazanim(
                    code=sub_code,
                    description=description,
                    parent_code=current_main,
                    grade=metadata['grade'],
                    subject=metadata['subject_code'],
                    semester=metadata['semester']
                ))

    return kazanimlar


# ============================================================================
# KEYWORD ENRICHMENT (LLM)
# ============================================================================

def get_llm():
    """Azure OpenAI LLM instance."""
    settings = get_settings()
    return AzureChatOpenAI(
        azure_endpoint=settings.azure_openai_endpoint,
        api_key=settings.azure_openai_api_key,
        api_version=settings.azure_openai_api_version,
        azure_deployment=settings.azure_openai_chat_deployment,
        temperature=0.3,
        max_tokens=500
    )


async def search_textbook_for_kazanim(
    kazanim: Kazanim,
    top_k: int = 5
) -> List[Dict[str, Any]]:
    """KazanÄ±m iÃ§in kitap index'inde arama yap."""
    settings = get_settings()
    client = get_search_client(settings.azure_search_index_kitap)

    search_text = f"{kazanim.description} {kazanim.code}"
    textbook_subject = get_textbook_subject(kazanim.subject)

    # Grade filter: KazanÄ±mÄ±n sÄ±nÄ±fÄ± ve bir Ã¶nceki sÄ±nÄ±f
    min_grade = max(9, kazanim.grade - 1)
    filter_str = f"grade ge {min_grade} and grade le {kazanim.grade} and subject eq '{textbook_subject}'"

    try:
        results = client.search(
            search_text=search_text,
            filter=filter_str,
            query_type="semantic",
            semantic_configuration_name="semantic-config",
            top=top_k,
            select=["content", "hierarchy_path", "page_range", "grade", "textbook_name"]
        )

        return [
            {
                "content": r.get("content", "")[:2000],
                "hierarchy_path": r.get("hierarchy_path", ""),
                "page_range": r.get("page_range", ""),
                "grade": r.get("grade"),
                "textbook": r.get("textbook_name", "")
            }
            for r in results
        ]
    except Exception as e:
        print(f"      âš ï¸ Kitap aramasÄ± hatasÄ±: {e}")
        return []


async def extract_keywords_with_llm(
    kazanim: Kazanim,
    textbook_contents: List[Dict[str, Any]],
    llm: AzureChatOpenAI
) -> List[str]:
    """LLM kullanarak kitap iÃ§eriklerinden anahtar kelimeleri Ã§Ä±kar."""
    if not textbook_contents:
        return []

    content_text = "\n\n---\n\n".join([
        f"[{c['hierarchy_path']} - Sayfa {c['page_range']}]\n{c['content']}"
        for c in textbook_contents
    ])

    prompt = f"""Sen bir MEB mÃ¼fredat uzmanÄ±sÄ±n. AÅŸaÄŸÄ±daki kazanÄ±m ve ilgili ders kitabÄ± iÃ§eriklerini analiz et.

KAZANIM:
Kod: {kazanim.code}
AÃ§Ä±klama: {kazanim.description}

DERS KÄ°TABI Ä°Ã‡ERÄ°KLERÄ°:
{content_text}

GÃ–REV:
Bu kazanÄ±mÄ± Ã¶ÄŸrencilerin sorularÄ±yla eÅŸleÅŸtirmek iÃ§in kullanÄ±lacak ANAHTAR KELÄ°MELER listesi oluÅŸtur.

KURALLAR:
1. Kitap iÃ§eriklerinden SOMUT terimler Ã§Ä±kar (Ã¶rn: "kalp", "kapakÃ§Ä±k", "bikÃ¼spit", "aort")
2. Soyut/genel terimlerden kaÃ§Ä±n (Ã¶rn: "sistem", "sÃ¼reÃ§", "model")
3. TÃ¼rkÃ§e ve Latince/bilimsel terimlerin ikisini de dahil et
4. 10-20 arasÄ±nda anahtar kelime ver
5. Kelimeler kÃ¼Ã§Ã¼k harfle, virgÃ¼lle ayrÄ±lmÄ±ÅŸ olsun

SADECE anahtar kelimeleri virgÃ¼lle ayÄ±rarak yaz, baÅŸka bir ÅŸey yazma.
Ã–rnek format: kalp, kapakÃ§Ä±k, bikÃ¼spit, trikÃ¼spit, aort, pulmoner"""

    try:
        response = await asyncio.to_thread(llm.invoke, prompt)
        keywords_text = response.content.strip()
        keywords = [k.strip().lower() for k in keywords_text.split(",")]
        keywords = [k for k in keywords if len(k) > 2]
        return keywords[:20]
    except Exception as e:
        print(f"      âš ï¸ LLM hatasÄ±: {e}")
        return []


async def enrich_kazanim(
    kazanim: Kazanim,
    llm: AzureChatOpenAI,
    verbose: bool = False
) -> bool:
    """
    Tek bir kazanÄ±mÄ± zenginleÅŸtir.
    Returns True if successful, False otherwise.
    """
    # 1. Kitap aramasÄ±
    textbook_results = await search_textbook_for_kazanim(kazanim)

    if not textbook_results:
        if verbose:
            print(f"      âš ï¸ Kitap iÃ§eriÄŸi bulunamadÄ±")
        return False

    # 2. LLM ile keyword extraction
    keywords = await extract_keywords_with_llm(kazanim, textbook_results, llm)

    if not keywords:
        if verbose:
            print(f"      âš ï¸ Keyword Ã§Ä±karÄ±lamadÄ±")
        return False

    # 3. KazanÄ±mÄ± gÃ¼ncelle
    kazanim.keywords = keywords
    kazanim.enriched_description = f"{kazanim.description} | Anahtar Kavramlar: {', '.join(keywords)}"
    kazanim.textbook_sources = [
        f"{r['textbook']} - {r['hierarchy_path']} (s.{r['page_range']})"
        for r in textbook_results
    ]

    if verbose:
        print(f"      âœ… {len(keywords)} keyword: {', '.join(keywords[:5])}...")

    return True


# ============================================================================
# INDEXING
# ============================================================================

def index_kazanimlar_batch(kazanimlar: List[Kazanim], use_enriched: bool = True) -> int:
    """KazanÄ±mlarÄ± Azure Search'e batch olarak indexle."""
    from src.vector_store.indexing_pipeline import IndexingPipeline

    pipeline = IndexingPipeline()

    docs = []
    for k in kazanimlar:
        # Use enriched description if available, otherwise original
        description = k.enriched_description if (use_enriched and k.enriched_description) else k.description

        docs.append({
            "id": sanitize_id(k.code),
            "code": k.code,
            "parent_code": k.parent_code,
            "description": description,
            "title": k.description,  # Original description as title
            "grade": k.grade,
            "subject": k.subject,
            "semester": k.semester
        })

    # Index
    if docs:
        pipeline.index_kazanimlar_raw(docs)

    return len(docs)


# ============================================================================
# MAIN PIPELINE
# ============================================================================

async def process_single_file(
    txt_path: Path,
    llm: Optional[AzureChatOpenAI],
    stats: PipelineStats,
    enrich: bool = True,
    verbose: bool = False
) -> List[Kazanim]:
    """Tek bir TXT dosyasÄ±nÄ± iÅŸle."""
    print(f"\nğŸ“„ {txt_path.name}")

    # 1. Parse metadata
    metadata = parse_metadata_from_filename(txt_path.name)
    print(f"   Ders: {metadata['subject']} ({metadata['subject_code']}), "
          f"SÄ±nÄ±f: {metadata['grade']}, DÃ¶nem: {metadata['semester']}")

    # 2. Parse kazanÄ±mlar
    kazanimlar = parse_txt_file(txt_path, metadata)
    main_count = len([k for k in kazanimlar if k.code == k.parent_code])
    sub_count = len(kazanimlar) - main_count
    print(f"   â””â”€ {main_count} ana + {sub_count} alt = {len(kazanimlar)} kazanÄ±m parse edildi")

    stats.total_kazanimlar += len(kazanimlar)

    # 3. Enrich with keywords (optional)
    if enrich and llm:
        print(f"   â””â”€ Keyword enrichment baÅŸlÄ±yor...")
        enriched = 0
        failed = 0

        for i, kazanim in enumerate(kazanimlar):
            if verbose:
                print(f"      [{i+1}/{len(kazanimlar)}] {kazanim.code}")

            success = await enrich_kazanim(kazanim, llm, verbose)
            if success:
                enriched += 1
            else:
                failed += 1

            # Rate limiting
            await asyncio.sleep(0.3)

            # Progress indicator
            if not verbose and (i + 1) % 10 == 0:
                print(f"      Ä°lerleme: {i+1}/{len(kazanimlar)}")

        print(f"   â””â”€ Enrichment: {enriched} baÅŸarÄ±lÄ±, {failed} baÅŸarÄ±sÄ±z")
        stats.enriched_count += enriched
        stats.failed_enrichment += failed

    return kazanimlar


async def run_pipeline(args):
    """Ana pipeline."""
    print("=" * 70)
    print("KAZANIM Ä°NDEXLEME PÄ°PELÄ°NE")
    print("TXT â†’ LLM Enrichment â†’ Azure Search")
    print("=" * 70)

    stats = PipelineStats()

    # 1. TXT dosyalarÄ±nÄ± bul
    input_dir = Path(args.input)
    if not input_dir.exists():
        print(f"\nâŒ KlasÃ¶r bulunamadÄ±: {input_dir}")
        print("Ã–nce extract_kazanimlar_to_txt.py Ã§alÄ±ÅŸtÄ±rÄ±n.")
        return

    txt_files = sorted(input_dir.glob("*_kazanimlar.txt"))

    # Subject filter
    if args.subject:
        subject_lower = args.subject.lower()
        txt_files = [f for f in txt_files if subject_lower in f.name.lower()]

    if not txt_files:
        print("\nâŒ TXT dosyasÄ± bulunamadÄ±!")
        return

    stats.total_files = len(txt_files)
    print(f"\nğŸ“š {len(txt_files)} TXT dosyasÄ± iÅŸlenecek")

    # 2. LLM hazÄ±rla (enrichment iÃ§in)
    llm = None
    if not args.no_enrich:
        print("\nğŸ¤– LLM hazÄ±rlanÄ±yor...")
        llm = get_llm()
        print("   â””â”€ âœ… Azure OpenAI baÄŸlantÄ±sÄ± kuruldu")
    else:
        print("\nâš¡ Enrichment atlanÄ±yor (--no-enrich)")

    # 3. Index reset (optional)
    if args.reset and not args.dry_run:
        print("\nğŸ—‘ï¸ KazanÄ±m indexi sÄ±fÄ±rlanÄ±yor...")
        from src.vector_store.indexing_pipeline import IndexingPipeline
        pipeline = IndexingPipeline()
        pipeline.delete_indexes("kazanim")
        pipeline.create_all_indexes()
        print("   â””â”€ âœ… Index yeniden oluÅŸturuldu")

    # 4. Her dosyayÄ± iÅŸle
    all_kazanimlar: List[Kazanim] = []

    for txt_path in txt_files:
        try:
            kazanimlar = await process_single_file(
                txt_path=txt_path,
                llm=llm,
                stats=stats,
                enrich=not args.no_enrich,
                verbose=args.verbose
            )
            all_kazanimlar.extend(kazanimlar)
        except Exception as e:
            print(f"   â””â”€ âŒ Hata: {e}")
            import traceback
            traceback.print_exc()

    # 5. Index
    if args.dry_run:
        print("\nğŸ” DRY RUN - Index iÅŸlemi yapÄ±lmayacak")
        print(f"   Toplam {len(all_kazanimlar)} kazanÄ±m indexlenecekti")

        # Show sample
        enriched_samples = [k for k in all_kazanimlar if k.keywords][:3]
        if enriched_samples:
            print("\n   Ã–rnek zenginleÅŸtirilmiÅŸ kazanÄ±mlar:")
            for k in enriched_samples:
                print(f"\n   {k.code}:")
                print(f"   Original: {k.description[:60]}...")
                print(f"   Keywords: {', '.join(k.keywords[:8])}...")
    else:
        print(f"\nğŸ“¤ Azure Search'e indexleniyor...")
        indexed = index_kazanimlar_batch(all_kazanimlar, use_enriched=not args.no_enrich)
        stats.indexed_count = indexed
        print(f"   â””â”€ âœ… {indexed} kazanÄ±m indexlendi")

    # 6. Rapor kaydet
    if not args.dry_run:
        report_path = Path(args.output)
        report_path.parent.mkdir(parents=True, exist_ok=True)

        report = {
            "timestamp": datetime.now().isoformat(),
            "stats": {
                "total_files": stats.total_files,
                "total_kazanimlar": stats.total_kazanimlar,
                "enriched_count": stats.enriched_count,
                "failed_enrichment": stats.failed_enrichment,
                "indexed_count": stats.indexed_count,
                "elapsed_seconds": stats.elapsed_seconds()
            },
            "kazanimlar": [
                {
                    "code": k.code,
                    "description": k.description,
                    "keywords": k.keywords,
                    "enriched": bool(k.keywords),
                    "grade": k.grade,
                    "subject": k.subject
                }
                for k in all_kazanimlar
            ]
        }

        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“„ Rapor kaydedildi: {report_path}")

    # 7. Ã–zet
    print("\n" + "=" * 70)
    print("Ã–ZET")
    print("=" * 70)
    print(f"Ä°ÅŸlenen dosya: {stats.total_files}")
    print(f"Toplam kazanÄ±m: {stats.total_kazanimlar}")
    if not args.no_enrich:
        print(f"ZenginleÅŸtirilen: {stats.enriched_count}")
        print(f"Enrichment baÅŸarÄ±sÄ±z: {stats.failed_enrichment}")
    if not args.dry_run:
        print(f"Ä°ndexlenen: {stats.indexed_count}")
    print(f"SÃ¼re: {stats.elapsed_seconds():.1f} saniye")
    print("=" * 70)


def main():
    parser = argparse.ArgumentParser(
        description="KazanÄ±m Ä°ndexleme Pipeline (TXT â†’ LLM Enrichment â†’ Azure Search)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ã–rnekler:
  python scripts/index_kazanimlar_pipeline.py                    # TÃ¼m pipeline
  python scripts/index_kazanimlar_pipeline.py --reset            # Ã–nce indexi sil
  python scripts/index_kazanimlar_pipeline.py --subject BÄ°Y      # Sadece biyoloji
  python scripts/index_kazanimlar_pipeline.py --no-enrich        # Enrichment atla (hÄ±zlÄ±)
  python scripts/index_kazanimlar_pipeline.py --dry-run          # Test modu
  python scripts/index_kazanimlar_pipeline.py --verbose          # DetaylÄ± Ã§Ä±ktÄ±

Pipeline AdÄ±mlarÄ±:
  1. TXT dosyalarÄ±nÄ± oku (data/kazanimlar_txt/)
  2. KazanÄ±mlarÄ± parse et
  3. Her kazanÄ±m iÃ§in kitap aramasÄ± yap
  4. LLM ile anahtar kelimeler Ã§Ä±kar
  5. ZenginleÅŸtirilmiÅŸ kazanÄ±mlarÄ± indexle
        """
    )

    parser.add_argument(
        "--input", "-i",
        type=str,
        default="data/kazanimlar_txt",
        help="TXT dosyalarÄ±nÄ±n klasÃ¶rÃ¼ (varsayÄ±lan: data/kazanimlar_txt)"
    )

    parser.add_argument(
        "--output", "-o",
        type=str,
        default="data/kazanim_pipeline_report.json",
        help="Rapor dosyasÄ± (varsayÄ±lan: data/kazanim_pipeline_report.json)"
    )

    parser.add_argument(
        "--subject", "-s",
        type=str,
        help="Sadece belirli ders (Ã¶rn: BÄ°Y, biyoloji)"
    )

    parser.add_argument(
        "--reset", "-r",
        action="store_true",
        help="Ã–nce kazanÄ±m indexini sil"
    )

    parser.add_argument(
        "--no-enrich",
        action="store_true",
        help="LLM keyword enrichment atla (hÄ±zlÄ± indexleme)"
    )

    parser.add_argument(
        "--dry-run", "-d",
        action="store_true",
        help="Index iÅŸlemi yapmadan test et"
    )

    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="DetaylÄ± Ã§Ä±ktÄ± gÃ¶ster"
    )

    args = parser.parse_args()
    asyncio.run(run_pipeline(args))


if __name__ == "__main__":
    main()
